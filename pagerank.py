# pagerank.py
from graphframes import GraphFrame
from pyspark.sql.functions import col
import pickle

# ==========================================
# 2. Core Logic (Graph Generation)
# ==========================================

def generate_graph(pages):
    ''' Compute the directed graph generated by wiki links.
    Parameters:
    -----------
      pages: RDD
        An RDD where each row consists of one wikipedia articles with 'id' and
        'anchor_text'.
    Returns:
    --------
      edges: RDD
        An RDD where each row represents an edge in the directed graph created by
        the wikipedia links. The first entry should the source page id and the
        second entry is the destination page id. No duplicates should be present.
      vertices: RDD
        An RDD where each row represents a vetrix (node) in the directed graph
        created by the wikipedia links. No duplicates should be present.
    '''

    # Create edges: Breaking all links into separate sides
    # Assuming structure: (source_id, [(dest_id, name), ...])
    edges = pages.flatMap(lambda x: [(x[0], page_id) for page_id, name in x[1]])

    # Remove duplicate edges
    edges = edges.distinct()

    # Create vertices: Extract all source IDs and all destination IDs
    vertices = pages.flatMap(lambda x: [(x[0],)] + [(page_id,) for page_id, name in x[1]])

    # Remove duplicate vertices
    vertices = vertices.distinct()

    return edges, vertices

def create_page_rank():
    """
    Main function to load data, build the graph, and run PageRank.
    """

    # Load data
    pages_links = parquetFile.limit(1000).select("id","anchor_text").rdd

    # Construct the graph
    edges, vertices = generate_graph(pages_links)

    # Prepare DataFrames for GraphFrames
    # Repartitioning for optimization
    edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src')
    verticesDF = vertices.toDF(['id']).repartition(4, 'id')

    # Create GraphFrame
    g = GraphFrame(verticesDF, edgesDF)

    # Run PageRank
    pr_results = g.pageRank(resetProbability=0.15, maxIter=6)

    # Extract and sort results
    pr = pr_results.vertices.select("id", "pagerank")
    pr = pr.sort(col('pagerank').desc())

    # Save results
    pr.repartition(1).write.mode('overwrite').csv('pr', compression="gzip")
    pagerank_dict = pr.rdd.collectAsMap()

    with open('pagerank.pkl', 'wb') as f:
        pickle.dump(pagerank_dict, f)

    # Show results
    pr.show()