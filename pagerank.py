# pagerank.py
from graphframes import GraphFrame
from pyspark.sql.functions import col
import pickle

# ==========================================
# 2. Core Logic (Graph Generation)
# ==========================================

def generate_graph(pages):
    ''' Compute the directed graph generated by wiki links.
    Parameters:
    -----------
      pages: RDD
        An RDD where each row consists of one wikipedia articles with 'id' and
        'anchor_text'.
    Returns:
    --------
      edges: RDD
        An RDD where each row represents an edge in the directed graph created by
        the wikipedia links. The first entry should the source page id and the
        second entry is the destination page id. No duplicates should be present.
      vertices: RDD
        An RDD where each row represents a vetrix (node) in the directed graph
        created by the wikipedia links. No duplicates should be present.
    '''

    # Create edges: Breaking all links into separate sides
    edges = pages.flatMap(lambda row: [(row[0], link.id) for link in row[1]])

    # Remove duplicate edges
    edges = edges.distinct()

    vertices = edges.flatMap(lambda x: [x[0], x[1]])  # returns the list of nodes
    vertices = vertices.distinct().map(lambda x: (x,))

    return edges, vertices

def create_page_rank(bucket_name,pages_links):
    """
    Main function to load data, build the graph, and run PageRank.
    """

    # construct the graph
    edges, vertices = generate_graph(pages_links)
    print("generate_graph success")

    # compute PageRank
    edgesDF = edges.toDF(['src', 'dst']).repartition(64, 'src')
    verticesDF = vertices.toDF(['id']).repartition(64, 'id')

    print("Caching DataFrames...")
    edgesDF.cache()
    verticesDF.cache()

    # Force computation to populate cache
    edge_count = edgesDF.count()
    vertex_count = verticesDF.count()
    print(f"Graph has {vertex_count} vertices and {edge_count} edges")

    g = GraphFrame(verticesDF, edgesDF)

    try:
        pr_results = g.pageRank(resetProbability=0.15, maxIter=3)
        print("PageRank computation completed!")

        # Extract results
        pr = pr_results.vertices.select("id", "pagerank")
        pr = pr.sort(col('pagerank').desc())

        # Cache before operations
        pr.cache()
        pr_count = pr.count()

        # Write to GCS - using mode('overwrite') to avoid conflicts
        pr.repartition(1).write.mode('overwrite').csv(f'gs://{bucket_name}/pr', compression="gzip")

        print("pr success")

        # Save as pickle
        pagerank_dict = pr.rdd.collectAsMap()

        with open('pagerank.pkl', 'wb') as f:
            pickle.dump(pagerank_dict, f)
            print("save PR pkl")

        # Show results
        pr.show(20)

    except Exception as e:
        print(f"ERROR during PageRank computation: {e}")
        import traceback
        traceback.print_exc()
        raise

    finally:
        # Cleanup
        print("Cleaning up cached data...")
        edgesDF.unpersist()
        verticesDF.unpersist()
        if 'pr' in locals():
            pr.unpersist()