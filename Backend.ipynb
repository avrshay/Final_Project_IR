{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\"><b>Our Backend</b></h1>"
      ],
      "metadata": {
        "id": "DThnJU8SzKb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2 align=\"center\"><b>Setup:</b></h2>"
      ],
      "metadata": {
        "id": "qyWCFO4t0I4g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rHER7BiTLq2",
        "outputId": "410093c7-d42c-417b-e48a-c99e560967fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME          PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE  SCHEDULED_STOP\r\n",
            "cluster-0016  GCE       2                                             RUNNING  us-central1-a\r\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc clusters list --region us-central1\n",
        "#The cluster that we used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLd2R4ZVTOWa",
        "outputId": "fcfe3659-f011-4627-b4bb-e0ac8317c46c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes\n",
        "#The two lines are used to install external Python libraries (libraries/packages) into our workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to4YiaY_zKb-"
      },
      "outputs": [],
      "source": [
        "#  Basic imports\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "from collections import *\n",
        "import nltk\n",
        "import pyspark\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "from google.cloud import storage\n",
        "from operator import itemgetter\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "from graphframes import *\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72eyOMjBWZnb",
        "outputId": "23aa6671-2621-48c9-d291-bde1b1e067e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 247882 Jan 10 20:25 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"
          ]
        }
      ],
      "source": [
        "# Checking the initialization script when starting the cluster\n",
        "!ls -l /usr/lib/spark/jars/graph*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzHeTFEzzXfy"
      },
      "outputs": [],
      "source": [
        "# Data Configuration & Loading\n",
        "\n",
        "bucket_name = '319134458_214906935'\n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths=[]\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "for b in blobs:\n",
        "    if b.name != 'graphframes.sh':\n",
        "        paths.append(full_path+b.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EugFKsGPXiYq"
      },
      "source": [
        "~ GCP setup is complete! ~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dniFq1dAcdQf"
      },
      "source": [
        "<h2 align=\"center\"><b>Creating the Inverted Index:</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHCDgtYifEMP",
        "outputId": "5c194137-ddad-4e56-add4-77a3ebf20e88"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# configuration:\n",
        "\n",
        "import hashlib\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Constants and Helpers\n",
        "NUM_BUCKETS = 124\n",
        "\n",
        "\n",
        "def _hash(s):\n",
        "  \"\"\"\n",
        "    Generates a 5-byte BLAKE2b hex digest for a given string.\n",
        "\n",
        "    Args:\n",
        "        s (str): The input string to hash.\n",
        "\n",
        "    Returns:\n",
        "        str: A 10-character hexadecimal string.\n",
        "    \"\"\"\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "\n",
        "def token2bucket_id(token):\n",
        "  \"\"\"\n",
        "    Maps a token to a specific bucket index using its hash value.\n",
        "\n",
        "    Args:\n",
        "        token (str): The token (word) to map.\n",
        "\n",
        "    Returns:\n",
        "        int: The bucket ID, ranging from 0 to NUM_BUCKETS - 1.\n",
        "    \"\"\"\n",
        "    return int(_hash(token), 16) % NUM_BUCKETS\n",
        "\n",
        "# Define Stopwords\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "\n",
        "\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5aJcQhWXn5r"
      },
      "outputs": [],
      "source": [
        "# read the entire corpus to an rdd, directly from Google Storage Bucket and use our code from Colab to construct an inverted index.\n",
        "parquetFile = spark.read.parquet(*paths)\n",
        "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc_tMmajXpNy",
        "outputId": "af6fe8d7-f59d-4c4f-e68a-9ec7dfa0ba3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "data": {
            "text/plain": [
              "6348910"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the number of wiki pages - should be more than 6M\n",
        "parquetFile.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwFHNrtYYHBE",
        "outputId": "9c246139-90b3-4a74-c899-0fe14a8425b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inverted_index_gcp.py\r\n"
          ]
        }
      ],
      "source": [
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAjlvsH41xx9"
      },
      "outputs": [],
      "source": [
        "# In Spark, our code runs on many machines simultaneously, so we need to physically send the files to everyone.\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0, SparkFiles.getRootDirectory())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9acbgI6qbRl-"
      },
      "outputs": [],
      "source": [
        "from inverted_index_gcp import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yLQg26je5nc"
      },
      "outputs": [],
      "source": [
        "# preprocessing functions:\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def partition_postings_and_write(postings, bucket_name):\n",
        "    ''' Partitions posting lists into buckets and writes to disk. '''\n",
        "    # Bucket assignment\n",
        "    buckets = postings.map(lambda x: (token2bucket_id(x[0]), x))\n",
        "\n",
        "    # Group By Key (term)\n",
        "    buckets = buckets.groupByKey()\n",
        "\n",
        "    # write to the disk\n",
        "    buckets = buckets.map(lambda x: InvertedIndex.write_a_posting_list(x, 'postings_gcp', bucket_name))\n",
        "\n",
        "    return buckets\n",
        "\n",
        "\n",
        "def word_count(text, id):\n",
        "    ''' Count the frequency of each word in `text` (tf) that is not included in\n",
        "    `all_stopwords` and return entries that will go into our posting lists.\n",
        "    '''\n",
        "\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "\n",
        "    filtered_tokens = [w for w in tokens if w not in all_stopwords]\n",
        "\n",
        "    map = Counter(filtered_tokens) #create a map {term : counter}\n",
        "\n",
        "    #creats the required tuples for each term in map\n",
        "    result = [(term, (id, count)) for term, count in map.items()]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "    ''' Returns a sorted posting list by wiki_id. '''\n",
        "    return sorted(unsorted_pl)\n",
        "\n",
        "\n",
        "def calculate_df(postings):\n",
        "    ''' Takes a posting list RDD and calculate the df for each token. '''\n",
        "    return postings.mapValues(len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LbKvfmmbqOg",
        "outputId": "4f008509-b987-44da-dfd8-431038ae15cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Inverted Index Pipeline:\n",
        "\n",
        "# word counts map\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
        "w2df = calculate_df(postings_filtered)\n",
        "w2df_dict = w2df.collectAsMap()\n",
        "# partition posting lists and write out\n",
        "_ = partition_postings_and_write(postings_filtered, bucket_name).collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNUBFMPucBq-"
      },
      "outputs": [],
      "source": [
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs[k].extend(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFE5J7N_cNl7",
        "outputId": "dd184ead-2a3a-4d65-d1b5-3c5d5ede2f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying file://index.pkl [Content-Type=application/octet-stream]...\n",
            "/ [1 files][ 18.5 MiB/ 18.5 MiB]                                                \n",
            "Operation completed over 1 objects/18.5 MiB.                                     \n"
          ]
        }
      ],
      "source": [
        "# Create inverted index instance\n",
        "inverted = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted.posting_locs = super_posting_locs\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted.df = w2df_dict\n",
        "# write the global stats out\n",
        "inverted.write_index('.', 'index')\n",
        "# upload to gs\n",
        "index_src = \"index.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_BUQqZfcU0V",
        "outputId": "ecfd9e6b-61e3-4cfa-8d29-e5890c0c14b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 18.49 MiB  2026-01-11T12:37:32Z  gs://319134458_214906935/postings_gcp/index.pkl\r\n",
            "TOTAL: 1 objects, 19384795 bytes (18.49 MiB)\r\n"
          ]
        }
      ],
      "source": [
        "!gsutil ls -lh $index_dst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4n-Os7Oekma"
      },
      "source": [
        "<h2 align=\"center\"><b>Creating DL for BM25 and ID-to-Title Map:</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxtBOEioTHzM",
        "outputId": "e7dc9ca0-f191-481d-e473-6430a4ff78b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#  Calculate Document Length (DL)\n",
        "\n",
        "DL_rdd = word_counts \\\n",
        "    .map(lambda x: (x[1][0], x[1][1])) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "dl_df = DL_rdd.toDF([\"id\", \"dl\"])\n",
        "\n",
        "dl_final = dl_df.groupBy(\"id\").sum(\"dl\").withColumnRenamed(\"sum(dl)\", \"dl\")\n",
        "dl_final = dl_final.sort(col('dl').desc())\n",
        "dl_final.repartition(1).write.csv(f'gs://{bucket_name}/dl', compression=\"gzip\", mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbFwk972THzM",
        "outputId": "4807499d-e167-4b8c-8205-8362e6f9d534"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "#  Save ID to Title Map:\n",
        "\n",
        "id_title_df = parquetFile.select(\"id\", \"title\").where(col(\"title\").isNotNull())\n",
        "id_title_df.repartition(1).write.csv(f'gs://{bucket_name}/id_to_title', compression=\"gzip\", mode=\"overwrite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOUibHyMchxI"
      },
      "source": [
        "<h2 align=\"center\"><b>PageRank:</b></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvY9Z922fZZd"
      },
      "outputs": [],
      "source": [
        "# Calculate PageRank:\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "import pickle\n",
        "\n",
        "def generate_graph(pages):\n",
        "    ''' Compute the directed graph generated by wiki links.\n",
        "    Parameters:\n",
        "    -----------\n",
        "      pages: RDD\n",
        "        An RDD where each row consists of one wikipedia articles with 'id' and\n",
        "        'anchor_text'.\n",
        "    Returns:\n",
        "    --------\n",
        "      edges: RDD\n",
        "        An RDD where each row represents an edge in the directed graph created by\n",
        "        the wikipedia links. The first entry should the source page id and the\n",
        "        second entry is the destination page id. No duplicates should be present.\n",
        "      vertices: RDD\n",
        "        An RDD where each row represents a vetrix (node) in the directed graph\n",
        "        created by the wikipedia links. No duplicates should be present.\n",
        "    '''\n",
        "    # Page 12 (source) -> points to: [page 100, page 200, page 100] -> (12, 100), (12, 200), (12, 100)\n",
        "    # Divide pages into pairs:\n",
        "\n",
        "    # row[0] = source_id\n",
        "    # row[1] = anchor_text\n",
        "\n",
        "    # creats edges\n",
        "    edges = pages.flatMap(lambda row: [(row[0], link.id) for link in row[1]])\n",
        "    edges = edges.distinct()  # Multiple links from page A to page B need to be represented by a single edge (edges are not weighted)\n",
        "\n",
        "    # creats vertices (nodes)\n",
        "\n",
        "    vertices = edges.flatMap(lambda x: [x[0], x[1]])  # returns the list of nodes\n",
        "    vertices = vertices.distinct().map(\n",
        "        lambda x: (x,))  # converts each unique id to a tuple ((id1),(id2)) for mapping later\n",
        "\n",
        "    return edges, vertices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwT4DuhXdQTd",
        "outputId": "20a66beb-222d-42d1-b516-3d244562589f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 76:================================================>     (112 + 8) / 124]\r"
          ]
        }
      ],
      "source": [
        "pages_links =spark.read.parquet(\"GS://319134458_214906935/multistream*\").select(\"id\",\n",
        "\"anchor_text\").rdd\n",
        "\n",
        "# construct the graph\n",
        "edges, vertices = generate_graph(pages_links)\n",
        "# compute PageRank\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr = pr.sort(col('pagerank').desc())\n",
        "pr.repartition(1).write.csv(f'gs://{bucket_name}/pr', compression=\"gzip\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}