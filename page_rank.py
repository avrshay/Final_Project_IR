

def generate_graph(pages):
  ''' Compute the directed graph generated by wiki links.
  Parameters:
  -----------
    pages: RDD
      An RDD where each row consists of one wikipedia articles with 'id' and
      'anchor_text'.
  Returns:
  --------
    edges: RDD
      An RDD where each row represents an edge in the directed graph created by
      the wikipedia links. The first entry should the source page id and the
      second entry is the destination page id. No duplicates should be present.
    vertices: RDD
      An RDD where each row represents a vetrix (node) in the directed graph
      created by the wikipedia links. No duplicates should be present.
  '''
    # YOUR CODE HERE
  edges=pages.flatMap(lambda x: [(x[0],page_id) for page_id, name in x[1]]) #Breaking all links into a separate side
  edges=edges.distinct()
  vertices=pages.flatMap(lambda x: [(x[0],)]+[(page_id,) for page_id, name in x[1]]) #all page
  vertices=vertices.distinct()
  return edges,vertices

def create_page_rank():
    path="" # change!!!
    pages_links = spark.read.parquet(path).limit(1000).select("id", "anchor_text").rdd
    # construct the graph
    edges, vertices = generate_graph(pages_links)
    # compute PageRank
    edgesDF = edges.toDF(['src', 'dst']).repartition(4, 'src') #change gcp
    verticesDF = vertices.toDF(['id']).repartition(4, 'id')
    g = GraphFrame(verticesDF, edgesDF)
    pr_results = g.pageRank(resetProbability=0.15, maxIter=6)
    pr = pr_results.vertices.select("id", "pagerank")
    pr = pr.sort(col('pagerank').desc())
    # pr.repartition(1).write.csv('pr', compression="gzip")
    pr.repartition(1).write.mode('overwrite').csv('pr', compression="gzip")
